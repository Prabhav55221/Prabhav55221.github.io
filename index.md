---
layout: page
---

# hello!

<img src="https://Prabhav55221.github.io/profile.png" class="floatpic">

I'm **Prabhav Singh** *(pronounced **Prah-bav**)*.

<br>

I‚Äôm currently pursuing my **Master‚Äôs in Computer Science (Thesis)** with a specialization in [Human Language Technologies](https://www.clsp.jhu.edu/human-language-technology-masters/) at [Johns Hopkins University](https://engineering.jhu.edu), where I conduct research at the [Center for Language and Speech Processing (CLSP)](https://www.clsp.jhu.edu/). I‚Äôm fortunate to be advised by [Prof. Jason Eisner](https://www.cs.jhu.edu/~jason/) and [Prof. Jesus Villalba](https://engineering.jhu.edu/faculty/jesus-villalba/). Before this, I earned my **Bachelor‚Äôs in Electrical Engineering** from [Delhi University](https://www.du.ac.in), where I worked with [Prof. K.P.S. Rana](https://sites.google.com/site/kpsrana1/home) and [Prof. Vineet Kumar](http://nsut.ac.in/en/node/554) at the APC Lab, NSIT.

<br>

You can find more details in my [CV](https://Prabhav55221.github.io/file/prabhavsresume.pdf). Feel free to reach out at: `psingh at jhu dot edu`

> üí° <span style="color:#990000;"><strong>I am actively seeking PhD positions for Fall 2026.</strong> If you are aware of openings or opportunities, I'd deeply appreciate hearing from you!</span>

---

## my (ever-changing) research interests

<details open>
<summary><strong>Cheaper LLM + Human Workflows</strong>: Combining humans and LLMs in a pipeline that enables principled, cost-effective annotation and evaluation.</summary>

<br>

LLMs are increasingly employed as surrogate annotators and evaluators in NLP workflows. However, current practices often involve multiple heuristic decisions to design effective workflows. For example, choosing the appropriate subset to annotate, either by LLMs or humans, remains a costly decision.

Recently, I've been working on **AnnotationArena** ‚Äî an end-to-end framework to streamline LLM-based evaluation and annotation. This includes:

- Using *Value of Information* ([ref](https://arxiv.org/abs/2110.13973), [ref](https://dl.acm.org/doi/10.5555/2051237.2051240)) for inference-time decision making.
- Leveraging **gradient-based heuristics** ([ref](https://arxiv.org/abs/2002.08484), [ref](https://arxiv.org/abs/2402.04333)) for active learning.

I'm also exploring how to extend these approaches using **reinforcement learning** and **alignment techniques**, aiming for adaptive, continuous annotation pipelines with principled decisions.  
Additional interests include different labeling strategies like **ratings**, **ranking**, and **ordinal classification**.

</details>

---

<details>
<summary><strong>Multimodal Learning for Language and Speech</strong>: Fusing audio, text, and vision to solve tasks that are natural for humans ‚Äî but hard for machines.</summary>

<br>

I build models that integrate **speech, text, and vision** for tasks like:

- **Emotion Recognition** ([example](https://Prabhav55221.github.io/file/EmoJudge_Interspeech_CameraReady.pdf))
- **Speaker Diarization** ([example](https://Prabhav55221.github.io/file/CYS_MYD_CameraReady.pdf))
- **Speaker Recognition** ([example](https://Prabhav55221.github.io/file/nistsre.pdf))

I aim to learn from **heterogeneous modalities** with **minimal supervision**, especially for socially grounded tasks.

I‚Äôve developed some expertise in **emotion recognition**, which was my first research focus. Lately, I've shifted toward **speaker recognition** and **diarization**. I find diarization especially compelling ‚Äî it‚Äôs a core speech task with many unsolved challenges, particularly in **cross-modal and real-world** settings.

</details>

---

<details>
<summary><strong>ML Learning Theory / Replicability</strong>: Understanding how adaptive decisions affect replicability in transfer learning.</summary>

<br>

Thanks to coursework at JHU, I‚Äôve developed an interest in replicability theory ‚Äî which differs from reproducibility. See: [Replicability vs. Reproducibility](https://arxiv.org/abs/2201.08430), [Replicability Bounds](https://arxiv.org/abs/2305.15284)

My recent work focuses on:

- Deriving **replicability bounds** for **transfer learning**.
- Studying how **adaptive data selection** affects replicability in transfer learning pipelines.

See our draft manuscript: [Sensitivity of Selectivity in Transfer Learning](https://Prabhav55221.github.io/file/SensitivityOfSelectivity.pdf) *(in progress)*

</details>

---

> üìù See my [publications](https://Prabhav55221.github.io/publications) for more.

---

## üì¢ recent updates

- **June 2025**  
  Starting my summer internship at a **stealth startup** in California ‚Äî working on agentic workflows for document understanding in finance and retail.

- **May 2025**  
  Two papers accepted at **INTERSPEECH 2025**! (See [this post](https://x.com/psingh522/status/1925354318988751117)). Excited to present in Rotterdam üá≥üá±!

- **April 2025**  
  Our poster on LLM + Human collaboration ([read here](https://Prabhav55221.github.io/file/MASCSLL-FINAL.pdf)) won **Best Poster** at [MASC-SLL 2025](https://www.mascsll.org/program/#:~:text=Active%20Learning%20and%20Feature%2DAcquisition%20with%20LLMs%20and%20Humans%20(Prabhav%20Singh%2C%20Haojun%20Shi%2C%20Jason%20Eisner)).

- **September 2024**  
  Our **ICMI‚Äô24** paper on multimodal emotion recognition for **Mild Cognitive Impairment (MCI)** is now [available](https://dl.acm.org/doi/10.1145/3678957.3689332).

<br>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/MASCSLL2025?src=hash&amp;ref_src=twsrc%5Etfw">#MASCSLL2025</a> was super fun! Thanks to <a href="https://twitter.com/penn_state?ref_src=twsrc%5Etfw">@penn_state</a> for organising <a href="https://twitter.com/MASC_Conference?ref_src=twsrc%5Etfw">@MASC_Conference</a> (and for the ice-cream). <br><br>Our paper also won a best-poster award! <a href="https://t.co/1AlPB6iRG4">pic.twitter.com/1AlPB6iRG4</a></p>&mdash; Prabhav Singh (@psingh522) <a href="https://twitter.com/psingh522/status/1908705799276277926?ref_src=twsrc%5Etfw">April 6, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

