---
layout: page
---

# About Me

<img src="https://Prabhav55221.github.io/profile.png" class="floatpic">

Hi! I'm **Prabhav Singh (pronounced Prah-bav)**.<br><br>

I am currently pursuing my Master's in CS (Thesis) with a specialization in [Human Language Technologies](https://www.clsp.jhu.edu/human-language-technology-masters/) from [JHU](https://engineering.jhu.edu). I am doing my graduate research at the [Center for Language and Speech Processing](https://www.clsp.jhu.edu/) where I am fortunate to be advised by [Prof. Jason Eisner](https://www.cs.jhu.edu/~jason/) & [Prof. Jesus Villalba](https://engineering.jhu.edu/faculty/jesus-villalba/).

I completed my Bachelor's in Engineering (Electrical) from [Delhi University](https://www.du.ac.in) in India. There, I worked under [Prof. KPS Rana](https://sites.google.com/site/kpsrana1/home) and [Prof. Vineet Kumar](http://nsut.ac.in/en/node/554) at the APC Lab, NSIT during my undergraduate degree, where I researched ways to make language and speech models more humanistic. I had the privilege of working with [Prof. Debarka Sengupta](https://www.thesenguptalab.com) at the IIIT Delhi Speech Research Laboratory - Where we worked on a new approach to Question-Answering Systems.<br><br>

You can find all the boring stuff in my [CV](https://Prabhav55221.github.io/file/prabhavsresume.pdf), you can get in touch with me at my email ```psingh at jhu dot edu```. 

**<font color="#990000">I am actively seeking a PhD position for 2026 Fall admission. If you have any information, please contact me. Thank you!</font>**

---

## Research Interests

- **Cheaper LLM Workflows**- Most recently, I have been working to develop better and cheaper annotation and evaluation frameworks that make use of LLMs. This involves invoking concepts of Value of Information, Gradient based strategies for faster VOI, active learning and more.
- **Multimodal Learning for Language and Speech** - I design models that fuse information from speech, text, and visual cues, targeting tasks such as emotion recognition and speaker diarization. Iâ€™m especially interested in building systems that reason over heterogeneous modalities with minimal supervision.
- **Structured Probabilistic Modeling** - My work explores statistical methods such as multitask learning, variational inference, and Bayesian reasoning to represent linguistic structure and temporal sequences in a robust, interpretable manner.

See my [papers](https://Prabhav55221.github.io/publications) for more detail.

---

## News and Updates

- **May 2025**: Two papers accepted to Interspeech 2025 (See my [X post](https://x.com/psingh522/status/1925354318988751117) for more details). Will share arXiv links once we get the camera ready version up. See you in Rotterdam!
- **April 2025**: Our poster ([read here](https://Prabhav55221.github.io/file/MASCSLL-FINAL.pdf)) won a best poster award at [MASC-SLL 2025](https://www.mascsll.org/program/#:~:text=Active%20Learning%20and%20Feature%2DAcquisition%20with%20LLMs%20and%20Humans%20(Prabhav%20Singh%2C%20Haojun%20Shi%2C%20Jason%20Eisner))!
- **September 2024**: Our paper which explores multimodality in emotion recogntion for MCI's has been accepted to ICMI'24 and is now [available](https://dl.acm.org/doi/10.1145/3678957.3689332)!

<br>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/MASCSLL2025?src=hash&amp;ref_src=twsrc%5Etfw">#MASCSLL2025</a> was super fun! Thanks to <a href="https://twitter.com/penn_state?ref_src=twsrc%5Etfw">@penn_state</a> for organising <a href="https://twitter.com/MASC_Conference?ref_src=twsrc%5Etfw">@MASC_Conference</a> (and for the ice-cream). <br><br>Our paper also won a best-poster award! <a href="https://t.co/1AlPB6iRG4">pic.twitter.com/1AlPB6iRG4</a></p>&mdash; Prabhav Singh (@psingh522) <a href="https://twitter.com/psingh522/status/1908705799276277926?ref_src=twsrc%5Etfw">April 6, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

