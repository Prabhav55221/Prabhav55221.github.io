---
layout: page
---

# hello!

<img src="https://Prabhav55221.github.io/profile.png" class="floatpic">

I'm **Prabhav Singh** *(pronounced **Prah-bav**)*.

<br>

I’m currently pursuing my **Master’s in Computer Science (Thesis)** with a specialization in [Human Language Technologies](https://www.clsp.jhu.edu/human-language-technology-masters/) at [Johns Hopkins University](https://engineering.jhu.edu), where I conduct research at the [Center for Language and Speech Processing (CLSP)](https://www.clsp.jhu.edu/). I’m fortunate to be advised by [Prof. Jason Eisner](https://www.cs.jhu.edu/~jason/) and [Prof. Jesus Villalba](https://engineering.jhu.edu/faculty/jesus-villalba/). Before this, I earned my **Bachelor’s in Electrical Engineering** from [Delhi University](https://www.du.ac.in), where I worked with [Prof. K.P.S. Rana](https://sites.google.com/site/kpsrana1/home) and [Prof. Vineet Kumar](http://nsut.ac.in/en/node/554) at the APC Lab, NSIT.

<br>

You can find more details in my [CV](https://Prabhav55221.github.io/file/prabhavsresume.pdf). Feel free to reach out at: `psingh at jhu dot edu`

> 💡 <span style="color:#990000;"><strong>I am actively seeking PhD positions for Fall 2026.</strong> If you are aware of openings or opportunities, I'd deeply appreciate hearing from you!</span>

---

## my (ever-changing) research interests

My broad interests are in the field of langauge modeling, speech representation and any combination of both that is helpful in solving a task. See my <a href="https://Prabhav55221.github.io/publications">publications</a> for more, or read about my major research areas as of now below.

<details>
<summary><strong>Cheaper LLM + Human Workflows</strong>: Combining humans and LLMs in a pipeline that enables principled, cost-effective annotation and evaluation.</summary>

<br>

<p>
LLMs are increasingly employed as surrogate annotators and evaluators in NLP workflows. However, current practices often involve multiple heuristic decisions to design effective workflows. For example, choosing the appropriate subset to annotate — by LLMs or humans — remains a costly decision.
</p>

<p>
Recently, I've been working on <strong>AnnotationArena</strong> — an end-to-end framework to streamline LLM-based evaluation and annotation. This includes:
</p>

<ul>
  <li>Using <em>Value of Information</em> <a href="https://arxiv.org/abs/2110.13973">[1]</a>, <a href="https://dl.acm.org/doi/10.5555/2051237.2051240">[2]</a> for inference-time decision making.</li>
  <li>Leveraging <em>gradient-based heuristics</em> <a href="https://arxiv.org/abs/2002.08484">[3]</a>, <a href="https://arxiv.org/abs/2402.04333">[4]</a> for active learning.</li>
  <li>Exploring <strong>reinforcement learning</strong> and <strong>alignment techniques</strong> to enable adaptive, continuous annotation pipelines with principled decisions.</li>
</ul>

<p>
I'm also interested in alternative labeling strategies such as <strong>ratings</strong>, <strong>rankings</strong>, and <strong>ordinal classifications</strong>.
</p>

</details>

<hr>

<details>
<summary><strong>Multimodal Learning for Language and Speech</strong>: Fusing audio, text, and vision to solve tasks that are natural for humans — but hard for machines.</summary>

<br>

<p>
I build models that integrate <strong>speech, text, and vision</strong> for tasks like:
</p>

<ul>
  <li><strong>Emotion Recognition</strong> (<a href="https://Prabhav55221.github.io/file/EmoJudge_Interspeech_CameraReady.pdf">example</a>)</li>
  <li><strong>Speaker Diarization</strong> (<a href="https://Prabhav55221.github.io/file/CYS_MYD_CameraReady.pdf">example</a>)</li>
</ul>

<p>
My models learn from <strong>heterogeneous modalities</strong> with <em>minimal supervision</em>. I began my research journey with emotion recognition, and while I’ve developed a fair amount of expertise in that space, I’m increasingly drawn to <strong>speaker recognition</strong> and <strong>diarization</strong>. I find diarization particularly interesting — it's a fundamental speech task with open challenges in <em>temporal structure, multimodal fusion, and low-resource adaptation</em>. My recent works are focused on improving diarization quality through multitask learning and adaptive fusion.
</p>

</details>

<hr>

<details>
<summary><strong>ML Learning Theory / Replicability</strong>: Understanding how adaptive decisions affect replicability in transfer learning.</summary>

<br>

<p>
Thanks to rigorous theory coursework at JHU, I’ve developed a strong interest in <strong>replicability theory</strong> — distinct from reproducibility.
</p>

<p>
See <a href="https://arxiv.org/abs/2201.08430">[this]</a> and <a href="https://arxiv.org/abs/2305.15284">[this].</a>
</p>

<p>
My current research focuses on:
</p>

<ul>
  <li>Deriving <strong>replicability bounds</strong> for <strong>transfer learning</strong>.</li>
  <li>Investigating how <strong>adaptive data selection</strong> affects transferability and stability of learned models.</li>
</ul>

<p>
Read our ongoing manuscript: <a href="https://Prabhav55221.github.io/file/SensitivityOfSelectivity.pdf">Sensitivity of Selectivity in Transfer Learning</a> <em>(work-in-progress)</em>.
</p>

</details>


<!-- ## my (ever-changing) research interests

<details open>
<summary><strong>Cheaper LLM + Human Workflows</strong>: Combining humans and LLMs in a pipeline that enables principled, cost-effective annotation and evaluation.</summary>

<br>

LLMs are increasingly employed as surrogate annotators and evaluators in NLP workflows. However, current practices often involve multiple heuristic decisions to design effective workflows. For example, choosing the appropriate subset to annotate, either by LLMs or humans, remains a costly decision.

Recently, I've been working on **AnnotationArena** — an end-to-end framework to streamline LLM-based evaluation and annotation. This includes:

- Using *Value of Information* ([ref](https://arxiv.org/abs/2110.13973), [ref](https://dl.acm.org/doi/10.5555/2051237.2051240)) for inference-time decision making.
- Leveraging **gradient-based heuristics** ([ref](https://arxiv.org/abs/2002.08484), [ref](https://arxiv.org/abs/2402.04333)) for active learning.

I'm also exploring how to extend these approaches using **reinforcement learning** and **alignment techniques**, aiming for adaptive, continuous annotation pipelines with principled decisions.  
Additional interests include different labeling strategies like **ratings**, **ranking**, and **ordinal classification**.

</details>

---

<details>
<summary><strong>Multimodal Learning for Language and Speech</strong>: Fusing audio, text, and vision to solve tasks that are natural for humans — but hard for machines.</summary>

<br>

I build models that integrate **speech, text, and vision** for tasks like:

- **Emotion Recognition** ([example](https://Prabhav55221.github.io/file/EmoJudge_Interspeech_CameraReady.pdf))
- **Speaker Diarization** ([example](https://Prabhav55221.github.io/file/CYS_MYD_CameraReady.pdf))
- **Speaker Recognition** ([example](https://Prabhav55221.github.io/file/nistsre.pdf))

I aim to learn from **heterogeneous modalities** with **minimal supervision**, especially for socially grounded tasks.

I’ve developed some expertise in **emotion recognition**, which was my first research focus. Lately, I've shifted toward **speaker recognition** and **diarization**. I find diarization especially compelling — it’s a core speech task with many unsolved challenges, particularly in **cross-modal and real-world** settings.

</details>

---

<details>
<summary><strong>ML Learning Theory / Replicability</strong>: Understanding how adaptive decisions affect replicability in transfer learning.</summary>

<br>

Thanks to coursework at JHU, I’ve developed an interest in replicability theory — which differs from reproducibility. See: [Replicability vs. Reproducibility](https://arxiv.org/abs/2201.08430), [Replicability Bounds](https://arxiv.org/abs/2305.15284)

My recent work focuses on:

- Deriving **replicability bounds** for **transfer learning**.
- Studying how **adaptive data selection** affects replicability in transfer learning pipelines.

See our draft manuscript: [Sensitivity of Selectivity in Transfer Learning](https://Prabhav55221.github.io/file/SensitivityOfSelectivity.pdf) *(in progress)*

</details>

---

> 📝 See my [publications](https://Prabhav55221.github.io/publications) for more. -->

---

## 📢 recent updates

- **June 2025**  
  Starting my summer internship at a **stealth startup** in California — working on agentic workflows for document understanding in finance and retail.

- **May 2025**  
  Two papers accepted at **INTERSPEECH 2025**! (See [this post](https://x.com/psingh522/status/1925354318988751117)). Excited to present in Rotterdam 🇳🇱!

- **April 2025**  
  Our poster on LLM + Human collaboration ([read here](https://Prabhav55221.github.io/file/MASCSLL-FINAL.pdf)) won **Best Poster** at [MASC-SLL 2025](https://www.mascsll.org/program/#:~:text=Active%20Learning%20and%20Feature%2DAcquisition%20with%20LLMs%20and%20Humans%20(Prabhav%20Singh%2C%20Haojun%20Shi%2C%20Jason%20Eisner)).

- **September 2024**  
  Our **ICMI’24** paper on multimodal emotion recognition for **Mild Cognitive Impairment (MCI)** is now [available](https://dl.acm.org/doi/10.1145/3678957.3689332).

<br>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/MASCSLL2025?src=hash&amp;ref_src=twsrc%5Etfw">#MASCSLL2025</a> was super fun! Thanks to <a href="https://twitter.com/penn_state?ref_src=twsrc%5Etfw">@penn_state</a> for organising <a href="https://twitter.com/MASC_Conference?ref_src=twsrc%5Etfw">@MASC_Conference</a> (and for the ice-cream). <br><br>Our paper also won a best-poster award! <a href="https://t.co/1AlPB6iRG4">pic.twitter.com/1AlPB6iRG4</a></p>&mdash; Prabhav Singh (@psingh522) <a href="https://twitter.com/psingh522/status/1908705799276277926?ref_src=twsrc%5Etfw">April 6, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

