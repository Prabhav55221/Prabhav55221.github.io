---
layout: page
---

# hello!

<img src="https://Prabhav55221.github.io/profile.png" class="floatpic">

I'm **Prabhav Singh** *(pronounced **Prah-bav**)*.

<br>

Iâ€™m currently pursuing my **Masterâ€™s in Computer Science (Thesis)** with a specialization in [Human Language Technologies](https://www.clsp.jhu.edu/human-language-technology-masters/) at [Johns Hopkins University](https://engineering.jhu.edu), where I conduct research at the [Center for Language and Speech Processing (CLSP)](https://www.clsp.jhu.edu/). Iâ€™m fortunate to be advised by [Prof. Jason Eisner](https://www.cs.jhu.edu/~jason/) and [Prof. Jesus Villalba](https://engineering.jhu.edu/faculty/jesus-villalba/). Before this, I earned my **Bachelorâ€™s in Electrical Engineering** from [Delhi University](https://www.du.ac.in), where I worked with [Prof. K.P.S. Rana](https://sites.google.com/site/kpsrana1/home) and [Prof. Vineet Kumar](http://nsut.ac.in/en/node/554) at the APC Lab, NSIT.

<br>

You can find more details in my [CV](https://Prabhav55221.github.io/file/prabhavsresume.pdf). Feel free to reach out at: `psingh at jhu dot edu`

> ğŸ’¡ <span style="color:#990000;"><strong>I am actively seeking PhD positions for Fall 2026.</strong> If you are aware of openings or opportunities, I'd deeply appreciate hearing from you!</span>

---

## ğŸ§  my (ever-changing) research interests

- **Cheaper LLM Workflows**  
  Recently, I've been exploring **cost-efficient annotation and evaluation** using LLMs. This includes applying ideas from *Value of Information* ([ref](https://arxiv.org/abs/2110.13973), [ref](https://dl.acm.org/doi/10.5555/2051237.2051240)) and **gradient-based heuristics** ([ref](https://arxiv.org/abs/2002.08484), [ref](https://arxiv.org/abs/2402.04333)) to accelerate decision-making in human-in-the-loop systems.

- **Multimodal Learning for Language and Speech**  
  I build models that integrate **speech, text, and vision** for tasks like **emotion recognition** ([example](https://Prabhav55221.github.io/file/EmoJudge_Interspeech_CameraReady.pdf)) and **speaker diarization** ([example](https://Prabhav55221.github.io/file/CYS_MYD_CameraReady.pdf)). Iâ€™m especially interested in learning from **heterogeneous modalities** with minimal supervision.

- **Structured Probabilistic Modeling**  
  My work also explores **multitask learning**, **Bayesian inference**, and **temporal modeling**, with a focus on interpretable representations of linguistic structure.

> ğŸ“ See my [publications](https://Prabhav55221.github.io/publications) for more.

---

## ğŸ“¢ recent updates

- **ğŸ§ª June 2025**  
  Starting my summer internship at a **stealth startup** in California â€” working on agentic workflows for document understanding in finance and retail.

- **ğŸ“„ May 2025**  
  Two papers accepted at **INTERSPEECH 2025**! (See [this post](https://x.com/psingh522/status/1925354318988751117)). Excited to present in Rotterdam ğŸ‡³ğŸ‡±!

- **ğŸ† April 2025**  
  Our poster on LLM + Human collaboration ([read here](https://Prabhav55221.github.io/file/MASCSLL-FINAL.pdf)) won **Best Poster** at [MASC-SLL 2025](https://www.mascsll.org/program/#:~:text=Active%20Learning%20and%20Feature%2DAcquisition%20with%20LLMs%20and%20Humans%20(Prabhav%20Singh%2C%20Haojun%20Shi%2C%20Jason%20Eisner)).

- **ğŸ“š September 2024**  
  Our **ICMIâ€™24** paper on multimodal emotion recognition for **Mild Cognitive Impairment (MCI)** is now [available](https://dl.acm.org/doi/10.1145/3678957.3689332).

<br>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/MASCSLL2025?src=hash&amp;ref_src=twsrc%5Etfw">#MASCSLL2025</a> was super fun! Thanks to <a href="https://twitter.com/penn_state?ref_src=twsrc%5Etfw">@penn_state</a> for organising <a href="https://twitter.com/MASC_Conference?ref_src=twsrc%5Etfw">@MASC_Conference</a> (and for the ice-cream). <br><br>Our paper also won a best-poster award! <a href="https://t.co/1AlPB6iRG4">pic.twitter.com/1AlPB6iRG4</a></p>&mdash; Prabhav Singh (@psingh522) <a href="https://twitter.com/psingh522/status/1908705799276277926?ref_src=twsrc%5Etfw">April 6, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

