<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Marformer: A Transformer for Predicting Missing Data Distributions</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #ffffff;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
            padding: 80px 20px;
        }

        header {
            text-align: center;
            margin-bottom: 60px;
        }

        h1 {
            font-size: 3em;
            font-weight: 600;
            margin-bottom: 30px;
            color: #1a1a1a;
            line-height: 1.2;
        }

        .authors {
            font-size: 1.1em;
            color: #555;
            margin-bottom: 10px;
        }

        .affiliations {
            font-size: 0.95em;
            color: #777;
            margin-bottom: 40px;
            line-height: 1.8;
        }

        .affiliation-line {
            margin: 5px 0;
        }

        .buttons {
            display: flex;
            gap: 15px;
            justify-content: center;
            flex-wrap: wrap;
            margin-top: 40px;
        }

        .btn {
            display: inline-flex;
            align-items: center;
            gap: 10px;
            padding: 14px 28px;
            border-radius: 30px;
            text-decoration: none;
            font-weight: 500;
            font-size: 1em;
            transition: all 0.3s ease;
        }

        .btn-paper {
            background: #d0d0d0;
            color: #888;
            cursor: not-allowed;
        }

        .btn-code {
            background: #2d3748;
            color: white;
        }

        .btn-code:hover {
            background: #1a202c;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }

        .section {
            margin: 80px 0;
        }

        h2 {
            font-size: 2em;
            margin-bottom: 30px;
            color: #1a1a1a;
            text-align: center;
            font-weight: 600;
        }

        p {
            font-size: 1.1em;
            margin-bottom: 20px;
            color: #444;
            text-align: justify;
            line-height: 1.8;
        }

        .intro-text {
            max-width: 800px;
            margin: 0 auto;
            text-align: center;
        }

        .figure {
            margin: 50px 0;
            text-align: center;
        }

        .figure img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
        }

        .figure-caption {
            margin-top: 20px;
            font-size: 1em;
            color: #666;
            max-width: 800px;
            margin-left: auto;
            margin-right: auto;
            text-align: center;
        }

        .results-list {
            max-width: 800px;
            margin: 40px auto;
            text-align: left;
        }

        .results-list ul {
            list-style: none;
            padding: 0;
        }

        .results-list li {
            margin: 25px 0;
            padding-left: 30px;
            position: relative;
            font-size: 1.1em;
            line-height: 1.8;
            color: #444;
        }

        .results-list li:before {
            content: "â–¸";
            position: absolute;
            left: 0;
            color: #2d3748;
            font-weight: bold;
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 2em;
            }

            h2 {
                font-size: 1.5em;
            }

            .container {
                padding: 60px 15px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Marformer: A Transformer for Predicting<br>Missing Data Distributions</h1>

            <div class="authors">
                Prabhav Singh, Haojun Shi, Xiheng Wang, Jason Eisner
            </div>

            <div class="affiliations">
                <div class="affiliation-line">Center for Language and Speech Processing</div>
                <div class="affiliation-line">Johns Hopkins University, Baltimore MD</div>
            </div>

            <div class="buttons">
                <span class="btn btn-paper">
                    ðŸ“„ Paper (Coming Soon)
                </span>
                <a href="https://github.com/argolab/AnnotationArena" class="btn btn-code" target="_blank">
                    ðŸ’» Code
                </a>
            </div>
        </header>

        <div class="section">
            <h2>Introducing Marformer</h2>
            <div class="intro-text">
                <p>
                    We propose a <strong>Transformer-based architecture that directly predicts posterior marginal distributions</strong> for missing data without fitting an explicit generative model. Inspired by <em>BERT's masked language modeling</em>, our approach trains on artificially masked observed values to learn dependencies in the data. Unlike generative approaches that model joint distributions, we <strong>focus exclusively on the posterior marginals needed for downstream decisions</strong>, enabling more direct optimization and avoiding the complexity of full generative modeling.
                </p>
            </div>

            <div class="figure">
                <img src="images/marformer_main_figure.png" alt="Marformer Framework">
                <p class="figure-caption">
                    The design for the experiments presented in this paper. Each domain, parametrized by its attributes, acts as data source to generate Missing-at-Random (MAR) instances. The Marformer uses masked auto-encoding to get p<sub>Î¸</sub> that approximates the posterior marginals. We compare against baselines that are parametrized by learned q<sub>Ï†</sub> on the observed examples.
                </p>
            </div>
        </div>

        <div class="section">
            <h2>Architecture</h2>
            <div class="intro-text">
                <p>
                    The Marformer is a <strong>Transformer encoder</strong> that processes structured attributes through <em>compositional embeddings</em>. Each attribute's initial representation combines <strong>missingness indicators, type embeddings, observed values, and learned positional embeddings</strong>. Through multiple layers of cross-attention, the model <em>iteratively refines its predictions</em> by attending to other distributions, similar to message-passing algorithms but with learned refinement procedures.
                </p>
            </div>

            <div class="figure">
                <img src="images/marformer_architecture.png" alt="Marformer Architecture">
                <p class="figure-caption">
                    Architecture of Marformer, which is a Transformer encoder. The input is represented at layer 0 with unordered input tokens, one per attribute. Each token is transformed through L layers of cross-attention to yield refined predictions at the top layer.
                </p>
            </div>
        </div>

        <div class="section">
            <h2>Results</h2>
            <div class="results-list">
                <ul>
                    <li>
                        <strong>Bayesian Networks and Multivariate Gaussians:</strong> Marformer consistently outperforms EM methods across both MCAR and Sequential MAR settings, <em>even when EM has access to the true generative model structure</em>. Classical methods suffer from local optima, while Marformer achieves lower KL divergence with sufficient training data.
                    </li>
                    <li>
                        <strong>Real Dialogue Annotation Data:</strong> On HANNA and LLM-Rubric datasets, Marformer significantly outperforms the baseline MLP method (LLM-Rubric) in terms of log loss, RMSE, and correlation metrics (Pearson, Spearman, Kendall), demonstrating <em>superior ability to learn correlations between LLM and human annotators</em>.
                    </li>
                    <li>
                        <strong>Structured Ratings and Rankings:</strong> In the complex annotation domain with hierarchical ordinal ratings and Bradley-Terry pairwise rankings, Marformer outperforms the Stan MCMC baseline, <em>which has explicit knowledge of how ratings and rankings are generated</em>. Marformer achieves better predictive performance while being orders of magnitude faster.
                    </li>
                </ul>
            </div>
        </div>
    </div>
</body>
</html>
